{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_team06",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomyeom/2019_cau_oss_hackathon/blob/master/hackathon_team06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "is_mnist = False;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "d0f66bf4-25d4-44b5-e824-9ca682f6f6a6"
      },
      "source": [
        "'''\n",
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "if is_mnist == True:\n",
        "  input_shape = (x_train.shape[1],x_train.shape[2],1)\n",
        "  x_train_after = np.reshape(x_train_after,(60000,28,28,1))\n",
        "  x_test_after = np.reshape(x_test_after,(10000,28,28,1)) \n",
        "'''\n",
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "if is_mnist == True:\n",
        "  input_shape = (x_train.shape[1],x_train.shape[2],1)\n",
        "  x_train_after = np.reshape(x_train_after,(60000,28,28,1))\n",
        "  x_test_after = np.reshape(x_test_after,(10000,28,28,1))\n",
        "  \n",
        "  \n",
        "####  \n",
        "# Standardize images across the dataset, mean=0, stdev=1\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from matplotlib import pyplot\n",
        "from keras import backend as K\n",
        "K.set_image_dim_ordering('th')\n",
        "\n",
        "# convert from int to float\n",
        "x_train_after = x_train_after.astype('float32')\n",
        "x_test_after = x_test_after.astype('float32')\n",
        "# define data preparation\n",
        "\n",
        "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
        "#datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
        "#datagen = ImageDataGenerator(zca_whitening=True)\n",
        "\n",
        "# fit parameters from data\n",
        "datagen.fit(x_train_after)\n",
        "# configure batch size and retrieve one batch of images"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:940: UserWarning: Expected input to be images (as Numpy array) following the data format convention \"channels_first\" (channels on axis 1), i.e. expected either 1, 3 or 4 channels on axis 1. However, it was passed an array with shape (50000, 32, 32, 3) (32 channels).\n",
            "  ' channels).')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZ9n4V9kb5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "339edd81-d8c4-44d0-b6a7-f7ea821f094c"
      },
      "source": [
        "'''\n",
        "from keras import optimizers\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(16, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape = input_shape))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(16, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(keras.layers.Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(1500, activation='relu'))\n",
        "model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "#adam = optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-8, decay=0.0, amsgrad=False)\n",
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "cd_checkpoint = keras.callbacks.ModelCheckpoint(filepath=save_path +  'model_entire_' + data_type + '_' + team_name + '.h5', monitor='val_acc', verbose=1, save_best_only=True, save_weight_only=True)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 100, epochs = 100, shuffle=True, validation_data=[x_test_after, y_test], callbacks=[cd_checkpoint])\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "print('model recall')\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "print('weight save')\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "print('json save')\n",
        "'''"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom keras import optimizers\\n\\nmodel = keras.Sequential()\\n\\nmodel.add(keras.layers.Conv2D(16, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape = input_shape))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Conv2D(16, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(keras.layers.Dropout(0.2))\\n\\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(keras.layers.Dropout(0.3))\\n\\nmodel.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(keras.layers.Dropout(0.4))\\n\\nmodel.add(keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(keras.layers.Dropout(0.5))\\n\\nmodel.add(keras.layers.Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.Conv2D(256, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\\nmodel.add(keras.layers.BatchNormalization())\\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(keras.layers.Dropout(0.5))\\n\\nmodel.add(keras.layers.Flatten())\\n\\nmodel.add(keras.layers.Dense(1500, activation='relu'))\\nmodel.add(keras.layers.Dense(num_classes, activation='softmax'))\\n\\n# 모델 컴파일\\n# optimizer: 모델을 업데이트 하는 방식\\n# loss: 모델의 정확도를 판단하는 방식\\n# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\\n#adam = optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-8, decay=0.0, amsgrad=False)\\n\\nsave_path = '/content/'\\nteam_name = 'team06'\\ncd_checkpoint = keras.callbacks.ModelCheckpoint(filepath=save_path +  'model_entire_' + data_type + '_' + team_name + '.h5', monitor='val_acc', verbose=1, save_best_only=True, save_weight_only=True)\\n\\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\\n\\n# 모델 트레이닝\\n# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\\n# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\\n# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\\n# validation_data: 중간 성능 검증에 사용할 data set\\nmodel.fit(x_train_after, y_train, batch_size = 100, epochs = 100, shuffle=True, validation_data=[x_test_after, y_test], callbacks=[cd_checkpoint])\\n\\nmodel = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\\nprint('model recall')\\nmodel.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\\nprint('weight save')\\nmodel_json = model.to_json()\\nwith open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \\n    json_file.write(model_json)\\nprint('json save')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l85ett48yjjB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3cf0a022-ce8a-4199-fd3b-a1417386c339"
      },
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import LeakyReLU\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(32, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same', input_shape = input_shape))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Conv2D(32, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same', input_shape = input_shape))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(keras.layers.Conv2D(64, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(keras.layers.Conv2D(128, kernel_initializer='he_normal', kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.4))\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(500, kernel_initializer='he_normal'))\n",
        "model.add(keras.layers.LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(keras.layers.Dense(num_classes, kernel_initializer='he_normal', activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "adam = optimizers.Adam(lr=0.005,beta_1=0.9,beta_2=0.999,epsilon=None, decay=1e-6, amsgrad=False)\n",
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "cd_checkpoint = keras.callbacks.ModelCheckpoint(filepath=save_path +  'model_entire_' + data_type + '_' + team_name + '.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', learning_rate=0.001, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 100, epochs = 200, shuffle=True, validation_data=[x_test_after, y_test], callbacks=[cd_checkpoint])\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "print('model recall')\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "print('weight save')\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "print('json save')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0810 - acc: 0.3591\n",
            "Epoch 00001: val_acc improved from -inf to 0.45010, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 21s 419us/sample - loss: 0.0810 - acc: 0.3596 - val_loss: 0.0726 - val_acc: 0.4501\n",
            "Epoch 2/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.5114\n",
            "Epoch 00002: val_acc improved from 0.45010 to 0.58280, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 168us/sample - loss: 0.0628 - acc: 0.5117 - val_loss: 0.0567 - val_acc: 0.5828\n",
            "Epoch 3/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.5775\n",
            "Epoch 00003: val_acc improved from 0.58280 to 0.59640, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 168us/sample - loss: 0.0555 - acc: 0.5774 - val_loss: 0.0549 - val_acc: 0.5964\n",
            "Epoch 4/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.6212\n",
            "Epoch 00004: val_acc improved from 0.59640 to 0.67520, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0504 - acc: 0.6213 - val_loss: 0.0442 - val_acc: 0.6752\n",
            "Epoch 5/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.6523\n",
            "Epoch 00005: val_acc improved from 0.67520 to 0.68160, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0465 - acc: 0.6524 - val_loss: 0.0441 - val_acc: 0.6816\n",
            "Epoch 6/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.6775\n",
            "Epoch 00006: val_acc improved from 0.68160 to 0.70090, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0437 - acc: 0.6774 - val_loss: 0.0410 - val_acc: 0.7009\n",
            "Epoch 7/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.6959\n",
            "Epoch 00007: val_acc improved from 0.70090 to 0.73380, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0413 - acc: 0.6960 - val_loss: 0.0373 - val_acc: 0.7338\n",
            "Epoch 8/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.7142\n",
            "Epoch 00008: val_acc did not improve from 0.73380\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0392 - acc: 0.7144 - val_loss: 0.0398 - val_acc: 0.7134\n",
            "Epoch 9/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.7267\n",
            "Epoch 00009: val_acc improved from 0.73380 to 0.75480, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0376 - acc: 0.7270 - val_loss: 0.0348 - val_acc: 0.7548\n",
            "Epoch 10/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.7368\n",
            "Epoch 00010: val_acc improved from 0.75480 to 0.76350, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0364 - acc: 0.7369 - val_loss: 0.0334 - val_acc: 0.7635\n",
            "Epoch 11/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.7455\n",
            "Epoch 00011: val_acc improved from 0.76350 to 0.76720, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0351 - acc: 0.7455 - val_loss: 0.0327 - val_acc: 0.7672\n",
            "Epoch 12/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.7534\n",
            "Epoch 00012: val_acc improved from 0.76720 to 0.78180, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0341 - acc: 0.7534 - val_loss: 0.0307 - val_acc: 0.7818\n",
            "Epoch 13/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.7630\n",
            "Epoch 00013: val_acc did not improve from 0.78180\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0332 - acc: 0.7633 - val_loss: 0.0373 - val_acc: 0.7381\n",
            "Epoch 14/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.7702\n",
            "Epoch 00014: val_acc improved from 0.78180 to 0.78220, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0322 - acc: 0.7702 - val_loss: 0.0307 - val_acc: 0.7822\n",
            "Epoch 15/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.7729\n",
            "Epoch 00015: val_acc did not improve from 0.78220\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0317 - acc: 0.7729 - val_loss: 0.0321 - val_acc: 0.7724\n",
            "Epoch 16/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.7815\n",
            "Epoch 00016: val_acc did not improve from 0.78220\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0307 - acc: 0.7814 - val_loss: 0.0321 - val_acc: 0.7744\n",
            "Epoch 17/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.7878\n",
            "Epoch 00017: val_acc improved from 0.78220 to 0.79330, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0300 - acc: 0.7880 - val_loss: 0.0304 - val_acc: 0.7933\n",
            "Epoch 18/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.7907\n",
            "Epoch 00018: val_acc improved from 0.79330 to 0.80480, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0296 - acc: 0.7906 - val_loss: 0.0278 - val_acc: 0.8048\n",
            "Epoch 19/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.7929\n",
            "Epoch 00019: val_acc improved from 0.80480 to 0.81070, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0292 - acc: 0.7930 - val_loss: 0.0270 - val_acc: 0.8107\n",
            "Epoch 20/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.8007\n",
            "Epoch 00020: val_acc did not improve from 0.81070\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0285 - acc: 0.8007 - val_loss: 0.0404 - val_acc: 0.7230\n",
            "Epoch 21/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.8007\n",
            "Epoch 00021: val_acc did not improve from 0.81070\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0283 - acc: 0.8006 - val_loss: 0.0284 - val_acc: 0.8016\n",
            "Epoch 22/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.8037\n",
            "Epoch 00022: val_acc did not improve from 0.81070\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0277 - acc: 0.8037 - val_loss: 0.0284 - val_acc: 0.8054\n",
            "Epoch 23/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.8077\n",
            "Epoch 00023: val_acc did not improve from 0.81070\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0274 - acc: 0.8076 - val_loss: 0.0286 - val_acc: 0.8043\n",
            "Epoch 24/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0269 - acc: 0.8104\n",
            "Epoch 00024: val_acc improved from 0.81070 to 0.81300, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0270 - acc: 0.8103 - val_loss: 0.0269 - val_acc: 0.8130\n",
            "Epoch 25/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.8159\n",
            "Epoch 00025: val_acc did not improve from 0.81300\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0265 - acc: 0.8157 - val_loss: 0.0294 - val_acc: 0.7976\n",
            "Epoch 26/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.8180\n",
            "Epoch 00026: val_acc did not improve from 0.81300\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0261 - acc: 0.8178 - val_loss: 0.0271 - val_acc: 0.8122\n",
            "Epoch 27/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.8192\n",
            "Epoch 00027: val_acc did not improve from 0.81300\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0258 - acc: 0.8194 - val_loss: 0.0279 - val_acc: 0.8084\n",
            "Epoch 28/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.8234\n",
            "Epoch 00028: val_acc improved from 0.81300 to 0.81770, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0253 - acc: 0.8234 - val_loss: 0.0263 - val_acc: 0.8177\n",
            "Epoch 29/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.8225\n",
            "Epoch 00029: val_acc improved from 0.81770 to 0.82070, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0252 - acc: 0.8226 - val_loss: 0.0258 - val_acc: 0.8207\n",
            "Epoch 30/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.8260\n",
            "Epoch 00030: val_acc improved from 0.82070 to 0.82600, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0249 - acc: 0.8258 - val_loss: 0.0253 - val_acc: 0.8260\n",
            "Epoch 31/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0243 - acc: 0.8306\n",
            "Epoch 00031: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0243 - acc: 0.8306 - val_loss: 0.0269 - val_acc: 0.8139\n",
            "Epoch 32/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.8316\n",
            "Epoch 00032: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0242 - acc: 0.8316 - val_loss: 0.0273 - val_acc: 0.8138\n",
            "Epoch 33/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.8325\n",
            "Epoch 00033: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0240 - acc: 0.8325 - val_loss: 0.0264 - val_acc: 0.8190\n",
            "Epoch 34/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.8358\n",
            "Epoch 00034: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0236 - acc: 0.8358 - val_loss: 0.0256 - val_acc: 0.8232\n",
            "Epoch 35/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.8351\n",
            "Epoch 00035: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0234 - acc: 0.8351 - val_loss: 0.0306 - val_acc: 0.7951\n",
            "Epoch 36/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.8372\n",
            "Epoch 00036: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0231 - acc: 0.8372 - val_loss: 0.0273 - val_acc: 0.8141\n",
            "Epoch 37/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.8404\n",
            "Epoch 00037: val_acc did not improve from 0.82600\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0231 - acc: 0.8403 - val_loss: 0.0267 - val_acc: 0.8193\n",
            "Epoch 38/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.8434\n",
            "Epoch 00038: val_acc improved from 0.82600 to 0.83040, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0227 - acc: 0.8434 - val_loss: 0.0247 - val_acc: 0.8304\n",
            "Epoch 39/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.8398\n",
            "Epoch 00039: val_acc did not improve from 0.83040\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0227 - acc: 0.8398 - val_loss: 0.0249 - val_acc: 0.8285\n",
            "Epoch 40/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.8448\n",
            "Epoch 00040: val_acc improved from 0.83040 to 0.83070, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0224 - acc: 0.8448 - val_loss: 0.0249 - val_acc: 0.8307\n",
            "Epoch 41/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.8443\n",
            "Epoch 00041: val_acc did not improve from 0.83070\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0224 - acc: 0.8443 - val_loss: 0.0253 - val_acc: 0.8277\n",
            "Epoch 42/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.8502\n",
            "Epoch 00042: val_acc improved from 0.83070 to 0.83310, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0218 - acc: 0.8502 - val_loss: 0.0247 - val_acc: 0.8331\n",
            "Epoch 43/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.8497\n",
            "Epoch 00043: val_acc improved from 0.83310 to 0.83380, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0216 - acc: 0.8496 - val_loss: 0.0246 - val_acc: 0.8338\n",
            "Epoch 44/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.8514\n",
            "Epoch 00044: val_acc did not improve from 0.83380\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0214 - acc: 0.8513 - val_loss: 0.0268 - val_acc: 0.8191\n",
            "Epoch 45/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.8495\n",
            "Epoch 00045: val_acc did not improve from 0.83380\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0218 - acc: 0.8494 - val_loss: 0.0273 - val_acc: 0.8115\n",
            "Epoch 46/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.8541\n",
            "Epoch 00046: val_acc improved from 0.83380 to 0.83590, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 169us/sample - loss: 0.0211 - acc: 0.8540 - val_loss: 0.0241 - val_acc: 0.8359\n",
            "Epoch 47/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.8551\n",
            "Epoch 00047: val_acc did not improve from 0.83590\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0211 - acc: 0.8550 - val_loss: 0.0244 - val_acc: 0.8333\n",
            "Epoch 48/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.8531\n",
            "Epoch 00048: val_acc did not improve from 0.83590\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0213 - acc: 0.8529 - val_loss: 0.0276 - val_acc: 0.8109\n",
            "Epoch 49/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.8567\n",
            "Epoch 00049: val_acc did not improve from 0.83590\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0207 - acc: 0.8567 - val_loss: 0.0250 - val_acc: 0.8289\n",
            "Epoch 50/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.8592\n",
            "Epoch 00050: val_acc improved from 0.83590 to 0.83920, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0205 - acc: 0.8592 - val_loss: 0.0233 - val_acc: 0.8392\n",
            "Epoch 51/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.8592\n",
            "Epoch 00051: val_acc did not improve from 0.83920\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0204 - acc: 0.8592 - val_loss: 0.0262 - val_acc: 0.8235\n",
            "Epoch 52/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.8613\n",
            "Epoch 00052: val_acc improved from 0.83920 to 0.84210, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0202 - acc: 0.8613 - val_loss: 0.0232 - val_acc: 0.8421\n",
            "Epoch 53/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.8624\n",
            "Epoch 00053: val_acc did not improve from 0.84210\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0201 - acc: 0.8621 - val_loss: 0.0253 - val_acc: 0.8306\n",
            "Epoch 54/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.8631\n",
            "Epoch 00054: val_acc did not improve from 0.84210\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0200 - acc: 0.8631 - val_loss: 0.0244 - val_acc: 0.8379\n",
            "Epoch 55/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.8652\n",
            "Epoch 00055: val_acc did not improve from 0.84210\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0198 - acc: 0.8653 - val_loss: 0.0252 - val_acc: 0.8286\n",
            "Epoch 56/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.8646\n",
            "Epoch 00056: val_acc improved from 0.84210 to 0.84970, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0196 - acc: 0.8646 - val_loss: 0.0221 - val_acc: 0.8497\n",
            "Epoch 57/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.8637\n",
            "Epoch 00057: val_acc did not improve from 0.84970\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0199 - acc: 0.8637 - val_loss: 0.0241 - val_acc: 0.8362\n",
            "Epoch 58/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.8646\n",
            "Epoch 00058: val_acc did not improve from 0.84970\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0195 - acc: 0.8646 - val_loss: 0.0244 - val_acc: 0.8346\n",
            "Epoch 59/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.8667\n",
            "Epoch 00059: val_acc improved from 0.84970 to 0.85080, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 168us/sample - loss: 0.0195 - acc: 0.8667 - val_loss: 0.0224 - val_acc: 0.8508\n",
            "Epoch 60/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.8683\n",
            "Epoch 00060: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0192 - acc: 0.8683 - val_loss: 0.0244 - val_acc: 0.8372\n",
            "Epoch 61/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.8666\n",
            "Epoch 00061: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 167us/sample - loss: 0.0193 - acc: 0.8667 - val_loss: 0.0229 - val_acc: 0.8479\n",
            "Epoch 62/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.8694\n",
            "Epoch 00062: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0190 - acc: 0.8693 - val_loss: 0.0227 - val_acc: 0.8477\n",
            "Epoch 63/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.8712\n",
            "Epoch 00063: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0188 - acc: 0.8713 - val_loss: 0.0241 - val_acc: 0.8374\n",
            "Epoch 64/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.8685\n",
            "Epoch 00064: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0192 - acc: 0.8683 - val_loss: 0.0237 - val_acc: 0.8415\n",
            "Epoch 65/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.8707\n",
            "Epoch 00065: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0188 - acc: 0.8708 - val_loss: 0.0258 - val_acc: 0.8296\n",
            "Epoch 66/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.8742\n",
            "Epoch 00066: val_acc did not improve from 0.85080\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0185 - acc: 0.8742 - val_loss: 0.0225 - val_acc: 0.8484\n",
            "Epoch 67/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.8738\n",
            "Epoch 00067: val_acc improved from 0.85080 to 0.85100, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 166us/sample - loss: 0.0184 - acc: 0.8738 - val_loss: 0.0225 - val_acc: 0.8510\n",
            "Epoch 68/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.8743\n",
            "Epoch 00068: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0184 - acc: 0.8742 - val_loss: 0.0236 - val_acc: 0.8430\n",
            "Epoch 69/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.8754\n",
            "Epoch 00069: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0183 - acc: 0.8755 - val_loss: 0.0224 - val_acc: 0.8484\n",
            "Epoch 70/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.8755\n",
            "Epoch 00070: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0181 - acc: 0.8756 - val_loss: 0.0235 - val_acc: 0.8424\n",
            "Epoch 71/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.8762\n",
            "Epoch 00071: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0181 - acc: 0.8760 - val_loss: 0.0250 - val_acc: 0.8322\n",
            "Epoch 72/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.8759\n",
            "Epoch 00072: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0181 - acc: 0.8759 - val_loss: 0.0225 - val_acc: 0.8509\n",
            "Epoch 73/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.8781\n",
            "Epoch 00073: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0178 - acc: 0.8782 - val_loss: 0.0227 - val_acc: 0.8473\n",
            "Epoch 74/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.8809\n",
            "Epoch 00074: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0175 - acc: 0.8809 - val_loss: 0.0237 - val_acc: 0.8439\n",
            "Epoch 75/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.8803\n",
            "Epoch 00075: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0176 - acc: 0.8800 - val_loss: 0.0233 - val_acc: 0.8434\n",
            "Epoch 76/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.8788\n",
            "Epoch 00076: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 0.0178 - acc: 0.8789 - val_loss: 0.0250 - val_acc: 0.8340\n",
            "Epoch 77/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.8805\n",
            "Epoch 00077: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0175 - acc: 0.8805 - val_loss: 0.0229 - val_acc: 0.8468\n",
            "Epoch 78/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.8819\n",
            "Epoch 00078: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0174 - acc: 0.8817 - val_loss: 0.0239 - val_acc: 0.8411\n",
            "Epoch 79/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.8820\n",
            "Epoch 00079: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0173 - acc: 0.8819 - val_loss: 0.0228 - val_acc: 0.8459\n",
            "Epoch 80/200\n",
            "49600/50000 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.8833\n",
            "Epoch 00080: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0171 - acc: 0.8831 - val_loss: 0.0233 - val_acc: 0.8460\n",
            "Epoch 81/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.8818\n",
            "Epoch 00081: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0173 - acc: 0.8818 - val_loss: 0.0227 - val_acc: 0.8492\n",
            "Epoch 82/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.8826\n",
            "Epoch 00082: val_acc did not improve from 0.85100\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0171 - acc: 0.8826 - val_loss: 0.0232 - val_acc: 0.8461\n",
            "Epoch 83/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.8853\n",
            "Epoch 00083: val_acc improved from 0.85100 to 0.85250, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 168us/sample - loss: 0.0170 - acc: 0.8854 - val_loss: 0.0220 - val_acc: 0.8525\n",
            "Epoch 84/200\n",
            "49800/50000 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.8842\n",
            "Epoch 00084: val_acc did not improve from 0.85250\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0169 - acc: 0.8840 - val_loss: 0.0234 - val_acc: 0.8430\n",
            "Epoch 85/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.8855\n",
            "Epoch 00085: val_acc did not improve from 0.85250\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0167 - acc: 0.8854 - val_loss: 0.0238 - val_acc: 0.8434\n",
            "Epoch 86/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.8846\n",
            "Epoch 00086: val_acc did not improve from 0.85250\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 0.0170 - acc: 0.8847 - val_loss: 0.0228 - val_acc: 0.8491\n",
            "Epoch 87/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.8870\n",
            "Epoch 00087: val_acc did not improve from 0.85250\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0166 - acc: 0.8870 - val_loss: 0.0240 - val_acc: 0.8395\n",
            "Epoch 88/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.8868\n",
            "Epoch 00088: val_acc improved from 0.85250 to 0.85640, saving model to /content/model_entire_cifar10_team06.h5\n",
            "50000/50000 [==============================] - 8s 165us/sample - loss: 0.0166 - acc: 0.8868 - val_loss: 0.0215 - val_acc: 0.8564\n",
            "Epoch 89/200\n",
            "49700/50000 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.8878\n",
            "Epoch 00089: val_acc did not improve from 0.85640\n",
            "50000/50000 [==============================] - 8s 163us/sample - loss: 0.0166 - acc: 0.8876 - val_loss: 0.0221 - val_acc: 0.8533\n",
            "Epoch 90/200\n",
            "49900/50000 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.8891\n",
            "Epoch 00090: val_acc did not improve from 0.85640\n",
            "50000/50000 [==============================] - 8s 164us/sample - loss: 0.0163 - acc: 0.8891 - val_loss: 0.0216 - val_acc: 0.8526\n",
            "Epoch 91/200\n",
            "12300/50000 [======>.......................] - ETA: 5s - loss: 0.0159 - acc: 0.8922"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Flatten layer: 28 x 28 x 1 image를 784개의 1D vector input으로 변환\n",
        "model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "\n",
        "# 1st hidden layer: fully-connected layer\n",
        "# (# of inputs = 784, # of outputs = 512, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "\n",
        "# 2nd hidden layer: fully-connected layer \n",
        "# (# of inputs = 512, # of outputs = 256, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "\n",
        "# 3rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 256, # of outputs = 64, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "\n",
        "# 4rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 64, # of outputs = 32, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
        "\n",
        "# 5rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 32, # of outputs = 16, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "\n",
        "# Output layer: fully-connected layer \n",
        "# (# of inputs = 16, # of outputs = 10, actication fuction = softmax)\n",
        "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 128, epochs = 30, shuffle=True, validation_data=[x_test_after, y_test])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuPa7OQg1qKI",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "0c49ac40-d4e7-400e-e4ae-11b4078ecea6"
      },
      "source": [
        "\n",
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nsave_path = '/content/'\\nteam_name = 'team06'\\n\\n# 모델의 weight 값만 저장합니다.\\nmodel.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\\n\\n# 모델의 구조만을 저장합니다.\\nmodel_json = model.to_json()\\nwith open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \\n    json_file.write(model_json)\\n\\n# 트레이닝된 전체 모델을 저장합니다.\\nmodel.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d66a5a2-cabb-43df-e2a8-55b5a32caa14"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "model.summary()\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_160 (Conv2D)          (None, 32, 32, 16)        448       \n",
            "_________________________________________________________________\n",
            "batch_normalization_161 (Bat (None, 32, 32, 16)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_161 (Conv2D)          (None, 32, 32, 16)        2320      \n",
            "_________________________________________________________________\n",
            "batch_normalization_162 (Bat (None, 32, 32, 16)        64        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_135 (MaxPoolin (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_120 (Dropout)        (None, 16, 16, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_162 (Conv2D)          (None, 16, 16, 32)        4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_163 (Bat (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_163 (Conv2D)          (None, 16, 16, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_164 (Bat (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_136 (MaxPoolin (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_121 (Dropout)        (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_164 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_165 (Bat (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "conv2d_165 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_166 (Bat (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_137 (MaxPoolin (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_122 (Dropout)        (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_166 (Conv2D)          (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_167 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_167 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_168 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_138 (MaxPoolin (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_123 (Dropout)        (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_168 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_169 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_170 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_139 (MaxPoolin (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_124 (Dropout)        (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1500)              385500    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 10)                15010     \n",
            "=================================================================\n",
            "Total params: 1,583,246\n",
            "Trainable params: 1,581,262\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n",
            "10000/10000 [==============================] - 2s 232us/sample - loss: 0.0225 - acc: 0.8502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02246283275745809, 0.8502]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nN5QVwY-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}