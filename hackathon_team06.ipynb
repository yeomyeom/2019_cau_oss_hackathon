{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hackathon_team06",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeomyeom/2019_cau_oss_hackathon/blob/master/hackathon_team06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc",
        "colab_type": "text"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        " *  단, 사용할 데이터셋에 따라 is_mnist만 수정\n",
        "*   모든 구현은 [2. 데이터 전처리]와 [3. 모델 생성]에서만 진행\n",
        " *  데이터 전처리 후 트레이닝, 데이터 셋은 x_train_after, x_test_after 변수명을 유지해주세요.\n",
        " *  데이터셋이 달라져도 같은 모델 구조를 유지하여야함.\n",
        "*   [4. 모델 저장]과 [5. 모델 로드 및 평가]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *  트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        " *  team_name을 제외한 다른 부분은 수정하지 말 것\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임->모든 런타임 재설정\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  모델 구조 파일 (model_structure_teamXX.json)\n",
        " *  모델 weight 파일 (model_weight_teamXX.h5)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        "* 제출 기한: **오후 6시**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2019_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드 or 알고리즘이 적발될 경우\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  두 개의 데이터셋에 대해 다른 모델 구조를 사용한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1",
        "colab_type": "text"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "is_mnist = True;\n",
        "\n",
        "# 데이터셋 로드\n",
        "# x_train, y_train: 트레이닝 데이터 및 레이블\n",
        "# x_test, y_test: 테스트 데이터 및 레이블\n",
        "if is_mnist:\n",
        "  data_type = 'mnist'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data() # fashion MNIST 데이터셋인 경우,\n",
        "else:\n",
        "  data_type = 'cifar10'\n",
        "  (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # cifar10 데이터셋인 경우,\n",
        "\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수\n",
        "num_classes = y_test.shape[1]\n",
        "\n",
        "# 인풋 데이터 타입\n",
        "input_shape = x_test.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9c2KLDBIhNQ",
        "colab_type": "text"
      },
      "source": [
        "# **2. 데이터 전처리**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJNgjaHvIhSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0419e0e2-c9c6-4658-a56d-ca1e4d58c7de"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "x_train_after = x_train / 255.0\n",
        "x_test_after = x_test / 255.0\n",
        "if is_mnist == True:\n",
        "  input_shape = (x_train.shape[1],x_train.shape[2],1)\n",
        "  x_train_after = np.reshape(x_train_after,(60000,28,28,1))\n",
        "  x_test_after = np.reshape(x_test_after,(10000,28,28,1))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9",
        "colab_type": "text"
      },
      "source": [
        "# **3. 모델 생성**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZ9n4V9kb5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26f333a4-1799-4a1f-9672-bf4800cd038c"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(5,5), strides=(1,1), padding='same', activation='relu', input_shape = input_shape))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(2,2), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(1000, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 128, epochs = 100, shuffle=True, validation_data=[x_test_after, y_test])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4881 - acc: 0.8229 - val_loss: 0.3737 - val_acc: 0.8597\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3249 - acc: 0.8825 - val_loss: 0.2994 - val_acc: 0.8907\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2802 - acc: 0.8966 - val_loss: 0.2662 - val_acc: 0.9040\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.2496 - acc: 0.9076 - val_loss: 0.2563 - val_acc: 0.9057\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2287 - acc: 0.9149 - val_loss: 0.2512 - val_acc: 0.9049\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2137 - acc: 0.9196 - val_loss: 0.2381 - val_acc: 0.9130\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1985 - acc: 0.9254 - val_loss: 0.2288 - val_acc: 0.9167\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1866 - acc: 0.9314 - val_loss: 0.2274 - val_acc: 0.9206\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1726 - acc: 0.9358 - val_loss: 0.2159 - val_acc: 0.9231\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1617 - acc: 0.9398 - val_loss: 0.2204 - val_acc: 0.9206\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1503 - acc: 0.9427 - val_loss: 0.2219 - val_acc: 0.9233\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1410 - acc: 0.9470 - val_loss: 0.2273 - val_acc: 0.9253\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1338 - acc: 0.9499 - val_loss: 0.2258 - val_acc: 0.9262\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1257 - acc: 0.9523 - val_loss: 0.2168 - val_acc: 0.9265\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1156 - acc: 0.9562 - val_loss: 0.2290 - val_acc: 0.9276\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1094 - acc: 0.9588 - val_loss: 0.2272 - val_acc: 0.9281\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.1012 - acc: 0.9621 - val_loss: 0.2368 - val_acc: 0.9305\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0965 - acc: 0.9631 - val_loss: 0.2257 - val_acc: 0.9300\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0881 - acc: 0.9674 - val_loss: 0.2519 - val_acc: 0.9294\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0824 - acc: 0.9690 - val_loss: 0.2633 - val_acc: 0.9233\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0810 - acc: 0.9691 - val_loss: 0.2529 - val_acc: 0.9277\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0752 - acc: 0.9719 - val_loss: 0.2491 - val_acc: 0.9275\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0698 - acc: 0.9739 - val_loss: 0.2634 - val_acc: 0.9300\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0688 - acc: 0.9744 - val_loss: 0.2687 - val_acc: 0.9311\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0640 - acc: 0.9765 - val_loss: 0.2579 - val_acc: 0.9304\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0606 - acc: 0.9775 - val_loss: 0.2768 - val_acc: 0.9294\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0583 - acc: 0.9785 - val_loss: 0.2799 - val_acc: 0.9290\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0533 - acc: 0.9803 - val_loss: 0.3063 - val_acc: 0.9297\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0543 - acc: 0.9796 - val_loss: 0.2992 - val_acc: 0.9258\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0527 - acc: 0.9802 - val_loss: 0.3034 - val_acc: 0.9297\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0513 - acc: 0.9801 - val_loss: 0.2990 - val_acc: 0.9286\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0484 - acc: 0.9821 - val_loss: 0.3055 - val_acc: 0.9281\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0464 - acc: 0.9829 - val_loss: 0.3030 - val_acc: 0.9297\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0431 - acc: 0.9838 - val_loss: 0.3117 - val_acc: 0.9305\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0441 - acc: 0.9838 - val_loss: 0.3235 - val_acc: 0.9288\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0413 - acc: 0.9852 - val_loss: 0.3339 - val_acc: 0.9246\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0404 - acc: 0.9847 - val_loss: 0.3342 - val_acc: 0.9266\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0412 - acc: 0.9847 - val_loss: 0.3333 - val_acc: 0.9304\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0410 - acc: 0.9852 - val_loss: 0.3177 - val_acc: 0.9278\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0375 - acc: 0.9861 - val_loss: 0.3293 - val_acc: 0.9285\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0389 - acc: 0.9859 - val_loss: 0.3439 - val_acc: 0.9248\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0389 - acc: 0.9858 - val_loss: 0.3767 - val_acc: 0.9277\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0349 - acc: 0.9877 - val_loss: 0.3250 - val_acc: 0.9287\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0359 - acc: 0.9867 - val_loss: 0.3574 - val_acc: 0.9302\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 3s 54us/sample - loss: 0.0352 - acc: 0.9866 - val_loss: 0.3776 - val_acc: 0.9266\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0312 - acc: 0.9887 - val_loss: 0.3856 - val_acc: 0.9286\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0334 - acc: 0.9882 - val_loss: 0.3710 - val_acc: 0.9288\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0309 - acc: 0.9891 - val_loss: 0.3745 - val_acc: 0.9291\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0334 - acc: 0.9879 - val_loss: 0.3437 - val_acc: 0.9297\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0328 - acc: 0.9888 - val_loss: 0.3528 - val_acc: 0.9267\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0340 - acc: 0.9881 - val_loss: 0.3619 - val_acc: 0.9274\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0324 - acc: 0.9890 - val_loss: 0.3934 - val_acc: 0.9302\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0319 - acc: 0.9885 - val_loss: 0.4066 - val_acc: 0.9267\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0298 - acc: 0.9891 - val_loss: 0.3875 - val_acc: 0.9270\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0269 - acc: 0.9906 - val_loss: 0.4417 - val_acc: 0.9266\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0315 - acc: 0.9890 - val_loss: 0.3769 - val_acc: 0.9277\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0274 - acc: 0.9902 - val_loss: 0.3892 - val_acc: 0.9289\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0301 - acc: 0.9892 - val_loss: 0.3904 - val_acc: 0.9278\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0288 - acc: 0.9902 - val_loss: 0.3713 - val_acc: 0.9277\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0330 - acc: 0.9887 - val_loss: 0.3937 - val_acc: 0.9279\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0272 - acc: 0.9902 - val_loss: 0.4067 - val_acc: 0.9268\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0280 - acc: 0.9901 - val_loss: 0.3955 - val_acc: 0.9303\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0277 - acc: 0.9908 - val_loss: 0.4003 - val_acc: 0.9298\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0275 - acc: 0.9904 - val_loss: 0.3960 - val_acc: 0.9271\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0285 - acc: 0.9903 - val_loss: 0.4362 - val_acc: 0.9296\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0270 - acc: 0.9909 - val_loss: 0.4201 - val_acc: 0.9275\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0263 - acc: 0.9910 - val_loss: 0.4028 - val_acc: 0.9302\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0252 - acc: 0.9913 - val_loss: 0.4135 - val_acc: 0.9294\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0277 - acc: 0.9905 - val_loss: 0.4198 - val_acc: 0.9277\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0258 - acc: 0.9913 - val_loss: 0.4084 - val_acc: 0.9312\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0260 - acc: 0.9911 - val_loss: 0.4453 - val_acc: 0.9290\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0277 - acc: 0.9907 - val_loss: 0.4359 - val_acc: 0.9273\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0294 - acc: 0.9902 - val_loss: 0.4457 - val_acc: 0.9282\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0260 - acc: 0.9907 - val_loss: 0.4300 - val_acc: 0.9294\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0270 - acc: 0.9910 - val_loss: 0.4389 - val_acc: 0.9296\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0243 - acc: 0.9914 - val_loss: 0.4179 - val_acc: 0.9315\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0243 - acc: 0.9921 - val_loss: 0.4246 - val_acc: 0.9290\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0267 - acc: 0.9911 - val_loss: 0.4401 - val_acc: 0.9280\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0243 - acc: 0.9915 - val_loss: 0.4909 - val_acc: 0.9303\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0258 - acc: 0.9911 - val_loss: 0.4526 - val_acc: 0.9302\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0253 - acc: 0.9917 - val_loss: 0.4641 - val_acc: 0.9308\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0229 - acc: 0.9923 - val_loss: 0.4259 - val_acc: 0.9315\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0221 - acc: 0.9926 - val_loss: 0.4228 - val_acc: 0.9306\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0253 - acc: 0.9918 - val_loss: 0.4501 - val_acc: 0.9296\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0238 - acc: 0.9918 - val_loss: 0.4612 - val_acc: 0.9291\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0220 - acc: 0.9926 - val_loss: 0.4577 - val_acc: 0.9261\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0239 - acc: 0.9922 - val_loss: 0.4218 - val_acc: 0.9272\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0226 - acc: 0.9927 - val_loss: 0.4935 - val_acc: 0.9276\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0262 - acc: 0.9915 - val_loss: 0.4899 - val_acc: 0.9277\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0236 - acc: 0.9923 - val_loss: 0.4354 - val_acc: 0.9296\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0239 - acc: 0.9919 - val_loss: 0.4492 - val_acc: 0.9299\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0218 - acc: 0.9927 - val_loss: 0.4694 - val_acc: 0.9279\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0223 - acc: 0.9924 - val_loss: 0.4593 - val_acc: 0.9291\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0248 - acc: 0.9923 - val_loss: 0.4730 - val_acc: 0.9272\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0250 - acc: 0.9919 - val_loss: 0.4846 - val_acc: 0.9274\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0221 - acc: 0.9927 - val_loss: 0.4927 - val_acc: 0.9288\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0205 - acc: 0.9930 - val_loss: 0.5097 - val_acc: 0.9282\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0219 - acc: 0.9930 - val_loss: 0.4614 - val_acc: 0.9262\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0218 - acc: 0.9926 - val_loss: 0.4743 - val_acc: 0.9268\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0220 - acc: 0.9926 - val_loss: 0.4464 - val_acc: 0.9267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6a77b49e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60e326c6-f9bf-4ab8-c34f-2ac03059ff4d"
      },
      "source": [
        "\n",
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Flatten layer: 28 x 28 x 1 image를 784개의 1D vector input으로 변환\n",
        "model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "\n",
        "# 1st hidden layer: fully-connected layer\n",
        "# (# of inputs = 784, # of outputs = 512, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
        "\n",
        "# 2nd hidden layer: fully-connected layer \n",
        "# (# of inputs = 512, # of outputs = 256, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "\n",
        "# 3rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 256, # of outputs = 64, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "\n",
        "# 4rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 64, # of outputs = 32, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(32, activation=tf.nn.relu))\n",
        "\n",
        "# 5rd hidden layer: fully-connected layer \n",
        "# (# of inputs = 32, # of outputs = 16, actication fuction = relu)\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "\n",
        "# Output layer: fully-connected layer \n",
        "# (# of inputs = 16, # of outputs = 10, actication fuction = softmax)\n",
        "model.add(keras.layers.Dense(num_classes, activation=tf.nn.softmax))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set\n",
        "model.fit(x_train_after, y_train, batch_size = 128, epochs = 30, shuffle=True, validation_data=[x_test_after, y_test])\n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.6766 - acc: 0.7650 - val_loss: 0.4278 - val_acc: 0.8483\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3832 - acc: 0.8630 - val_loss: 0.4113 - val_acc: 0.8544\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3376 - acc: 0.8766 - val_loss: 0.3867 - val_acc: 0.8619\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.3128 - acc: 0.8851 - val_loss: 0.3596 - val_acc: 0.8699\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2928 - acc: 0.8917 - val_loss: 0.3587 - val_acc: 0.8713\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2775 - acc: 0.8970 - val_loss: 0.3352 - val_acc: 0.8793\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2640 - acc: 0.9018 - val_loss: 0.3364 - val_acc: 0.8790\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2541 - acc: 0.9048 - val_loss: 0.3540 - val_acc: 0.8751\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2446 - acc: 0.9083 - val_loss: 0.3384 - val_acc: 0.8804\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2313 - acc: 0.9134 - val_loss: 0.3355 - val_acc: 0.8867\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2231 - acc: 0.9162 - val_loss: 0.3509 - val_acc: 0.8826\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2181 - acc: 0.9190 - val_loss: 0.3279 - val_acc: 0.8887\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.2077 - acc: 0.9220 - val_loss: 0.3239 - val_acc: 0.8880\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.2023 - acc: 0.9241 - val_loss: 0.3337 - val_acc: 0.8909\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1971 - acc: 0.9270 - val_loss: 0.3363 - val_acc: 0.8939\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1877 - acc: 0.9292 - val_loss: 0.3509 - val_acc: 0.8928\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1833 - acc: 0.9309 - val_loss: 0.3353 - val_acc: 0.8915\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1739 - acc: 0.9346 - val_loss: 0.3498 - val_acc: 0.8919\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1712 - acc: 0.9359 - val_loss: 0.3617 - val_acc: 0.8907\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1671 - acc: 0.9379 - val_loss: 0.3481 - val_acc: 0.8897\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1597 - acc: 0.9389 - val_loss: 0.3502 - val_acc: 0.8959\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1543 - acc: 0.9418 - val_loss: 0.3664 - val_acc: 0.8897\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1513 - acc: 0.9431 - val_loss: 0.3645 - val_acc: 0.8943\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1399 - acc: 0.9473 - val_loss: 0.4122 - val_acc: 0.8894\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1376 - acc: 0.9476 - val_loss: 0.3783 - val_acc: 0.8966\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1384 - acc: 0.9482 - val_loss: 0.3694 - val_acc: 0.8966\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1323 - acc: 0.9499 - val_loss: 0.4004 - val_acc: 0.8910\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1276 - acc: 0.9512 - val_loss: 0.4263 - val_acc: 0.8933\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1233 - acc: 0.9539 - val_loss: 0.3791 - val_acc: 0.8994\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 2s 38us/sample - loss: 0.1211 - acc: 0.9549 - val_loss: 0.3906 - val_acc: 0.8964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff6a6d9b438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR",
        "colab_type": "text"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "# 모델의 weight 값만 저장합니다.\n",
        "model.save_weights(save_path + 'model_weight_' + data_type + '_' + team_name + '.h5')\n",
        "\n",
        "# 모델의 구조만을 저장합니다.\n",
        "model_json = model.to_json()\n",
        "with open(save_path + 'model_structure_' + data_type + '_' + team_name + '.json', 'w') as json_file : \n",
        "    json_file.write(model_json)\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_' + data_type + '_' + team_name + '.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B2BoRDZ7cFl",
        "colab_type": "text"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDBwxVUx7knQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ee5c8dfb-e886-4fc6-a722-d569d667e87c"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team06'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + data_type + '_' + team_name + '.h5')\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 114us/sample - loss: 0.4464 - acc: 0.9267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.44640260967258366, 0.9267]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-nN5QVwY-_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}